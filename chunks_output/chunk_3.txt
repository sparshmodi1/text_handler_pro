SPMDMPMDThe single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The
single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a
large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a
rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays),
few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the
most common type of parallel programs.

According to David A. Patterson and John L. Hennessy, "Some machines are hybrids of these categories, of course, but
this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is
also—perhaps because of its understandability—the most widely used scheme."[34]
DisadvantagesParallel computing can incur significant overhead in practice, primarily due to the costs associated
with merging data from multiple processes. Specifically, inter-process communication and synchronization can lead to
overheads that are substantially higher—often by two or more orders of magnitude—compared to processing the same data
on a single thread.[35][36][37] Therefore, the overall improvement should be carefully evaluated.GranularityBit-level
parallelismMain article: Bit-level parallelismTaiwania 3 of Taiwan, a parallel supercomputing device that joined
COVID-19 researchFrom the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the
1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of
information the processor can manipulate per cycle.[38] Increasing the word size reduces the number of instructions
the processor must execute to perform an operation on variables whose sizes are greater than the length of the word.
For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order
bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an
add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two
instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a
single instruction.Historically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit
microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a
standard in general-purpose computing for two decades. Not until the early 2000s, with the advent of x86-64
architectures, did 64-bit processors become commonplace.Instruction-level parallelismMain article: Instruction-level
parallelismA canonical processor without pipeline. It takes five clock cycles to complete one instruction and thus
the processor can issue subscalar performance (IPC = 0.2 < 1).

A computer program is, in essence, a stream of instructions executed by a processor. Without instruction-level
parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are
known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed
in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in
instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.[39]

A canonical five-stage pipelined processor. In the best case scenario, it takes one clock cycle to complete one
instruction and thus the processor can issue scalar performance (IPC = 1).

All modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different
action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to
N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC =
1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC
processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and
register write back (WB). The Pentium 4 processor had a 35-stage pipeline.[40]

A canonical five-stage pipelined processor with two execution units. In the best case scenario, it takes one clock
cycle to complete two instructions and thus the processor can issue superscalar performance (IPC = 2 > 1).

Most modern processors also have multiple execution units. They usually combine this feature with pipelining and thus
can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors.
Superscalar processors differ from multi-core processors in that the several execution units are not entire
processors (i.e. processing units). Instructions can be grouped together only if there is no data dependency between
them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming)
are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.

Task parallelismMain article: Task parallelismTask parallelisms is the characteristic of a parallel program that

"entirely different calculations can be performed on either the same or different sets of data".[41] This contrasts
with data parallelism, where the same calculation is performed on the same or different sets of data. Task
parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for
execution. The processors would then execute these sub-tasks concurrently and often cooperatively. Task parallelism

does not usually scale with the size of a problem.Superword level parallelismSuperword level parallelism is a
vectorization technique based on loop unrolling and basic block vectorization. It is distinct from loop vectorization
algorithms in that it can exploit parallelism of inline code, such as manipulating coordinates, color channels or in

loops unrolled by hand.[43]

HardwareMemory and communicationMain memory in a parallel computer is either shared memory (shared between all
processing elements in a single address space), or distributed memory (in which each processing element has its own

local address space).[44] Distributed memory refers to the fact that the memory is logically distributed, but often
implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the
two approaches, where the processing element has its own local memory and access to the memory on non-local
processors. Accesses to local memory are typically faster than accesses to non-local memory. On the supercomputers,
distributed shared memory space can be implemented using the programming model such as PGAS. This model allows
processes on one compute node to transparently access the remote memory of another compute node. All compute nodes
are also connected to an external shared memory system via high-speed interconnect, such as Infiniband, this external
shared memory system is known as burst buffer, which is typically built from arrays of non-volatile memory physically
distributed across multiple I/O nodes.

A logical view of a non-uniform memory access (NUMA) architecture. Processors in one directory can access that
directory's memory with less latency than they can access memory in the other directory's memory.
Computer architectures in which each element of main memory can be accessed with equal latency and bandwidth are
known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in

which the memory is not physically distributed. A system that does not have this property is known as a non-uniform
memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.

Computer systems make use of caches—small and fast memories located close to the processor which store temporary
copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties
with caches that may store the same value in more than one location, with the possibility of incorrect program
execution. These computers require a cache coherency system, which keeps track of cached values and strategically
purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping
track of which values are being accessed (and thus should be purged). Designing large, high-performance cache
coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer
architectures do not scale and distributed memory systems do.[44]

Processor–processor and processor–memory communication can be implemented in hardware in several ways, including via