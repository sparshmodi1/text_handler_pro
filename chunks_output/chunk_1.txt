A couple weeks ago there was this news about the Nano editor no longer being part of the GNU project. My first
reaction was, wow people still really care about an old editor which is a clone of an editor originally part of a
terminal based EMAIL CLIENT. Let’s say this again, “email client”. The notion of email client itself is gone at this
point, everything changed. And yet I read, on Hacker News, a number of people writing how they were often saved by
the availability of nano on random systems, doing system administrator tasks, for example. Nano is also how my son
wrote his first program in C. It’s an acceptable experience that does not require past experience editing files.
This is how I started to think about writing a text editor ways smaller than Nano itself. Just for fun, basically,
because I like and admire small programs.
How lame, useless, wasting of time is today writing an editor? We are supposed to write shiny incredible projects,
very cool, very complex, valuable stuff. But sometimes to make things without a clear purpose is refreshing. There
were also memories…
I remember my first experiences with the Commodore 16 in-ROM assembler, and all the home computers and BASIC
interpreters I used later in my child life. An editor is a fundamental connection between the human and the machine.
It allows the human to write something that the computer can interpret. The blinking of a square prompt is something
that many of us will never forget.
Well, all nice, but my time was very limited. A few hours across two weekends with programmed family activities and
meat to prepare for friends in long barbecue sessions. Maybe I could still write an editor on a few spare hours with
some trick. My goal was to write an editor which was very small, no curses, and with syntax highlighting. Something
usable, basically. That’s the deal.. It’s little stuff, but is already hard to write all this from scratch in a few
hours.But … wait, I actually wrote an editor in the past, is part of the LOAD81 project, a Lua based programming
environment for children. Maybe I can just reuse it… and instead of using SDL to write on the screen what about
sending VT100 escape sequences directly to the terminal? And I’ve code for this as well in linenoise, another toy
project that eventually found its place in some may other serious projects. So maybe mixing the two…
The first week Saturday morning I went to the sea, and it was great. Later my brother arrived from Edinburg to
Catania, and at the end of the day we were together in the garden with our laptops, trying to defend ourselves from
the 30 degrees that there were during the day, so I started to hack the first skeleton of the editor. The LOAD81 code
was quite modular, to take it away from the original project was a joke. I could kinda edit after a few hours, and it
was already time to go bed. The next day I worked again at it before leaving for the sea again. My 15yo sleeps till
1pm, as I did when I was 15yo in summertime after all, so I coded more in sprints of 30 minutes waiting for him to
get up, using the rest of the time to play with my wonderful daughter. Finally later in the Sunday night I tried to
fix all the remaining stuff.
Hey I remember that a few years ago to hack on a project was, to *hack* on it, full time for days. I’m old now, but
still young enough to write toy editors and consider it a serious business :-)
However life is hard, and Monday arrived. Real business, no time for toy projects, not even time to release what I
got during the weekend… It deserved some minimal polishing and a blog post.
I had to wait this Monday to put my hands on the “Kilo” editor again. It’s called Kilo because it has less than 1024
lines of code. The “cloc” utility, used in order to count the number of lines of code, signaled me I still had ~100
lines of space before reaching 1024 LOC, and a serious editor needs a “search” feature after all. So back to the
code, trying also to restructure and recomment it a bit, since you know, when you mix two projects pieces in a few
hours the risk is that the code quality is less than excellent.
Well, now it’s time to release it, end of this crazy project. Maybe somebody will use it as a starting point to write
a real editor, or maybe it could be used to write some new interesting CLI that goes over the usual REPL style model.
Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously.
[1] Large problems can often be divided into smaller ones, which can then be solved at the same time. There are
several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism
has long been employed in high-performance computing, but has gained broader interest due to the physical constraints
preventing frequency scaling.[2] As power consumption (and consequently heat generation) by computers has become a
concern in recent years,[3] parallel computing has become the dominant paradigm in computer architecture, mainly in
the form of multi-core processors.[4]
Parallelism vs concurrencyIn computer science, parallelism and concurrency are two different things: a parallel
program uses multiple CPU cores, each core performing a task independently. On the other hand, concurrency enables a
program to deal with multiple tasks even on a single CPU core; the core switches between tasks (i.e. threads) without
necessarily completing each one. A program can have both, neither or a combination of parallelism and concurrency
characteristics.Parallel computers can be roughly classified according to the level at which the hardware supports
parallelism, with multi-core and multi-processor computers having multiple processing elements within a single
machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel
computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism,
but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than
sequential ones,[6] because concurrency introduces several new classes of potential software bugs, of which race
conditions are the most common. Communication and synchronization between the different subtasks are typically some
of the greatest obstacles to getting optimal parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's
law, which states that it is limited by the fraction of time for which the parallelization can be utilised.
Background
Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is
constructed and implemented as a serial stream of instructions. These instructions are executed on a central
processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the
next one is executed.[7]

Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is
accomplished by breaking the problem into independent parts so that each processing element can execute its part of
the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a
single computer with multiple processors, several networked computers, specialized hardware, or any combination of
the above.[7] Historically parallel computing was used for scientific computing and the simulation of scientific
problems, particularly in the natural and engineering sciences, such as meteorology. This led to the design of
parallel hardware and software, and high performance computing.[8]
Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The
runtime of a program is equal to the number of instructions multiplied by the average time per instruction.
Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute
an instruction. An increase in frequency thus decreases runtime for all compute-bound programs.[9] However, power
consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock
cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency
(cycles per second).[10] Increases in frequency increase the amount of power used in a processor. Increasing
processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors,
which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.[11]
To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor)
manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of
the processor and in multi-core processors each core is independent and can access the same memory concurrently.
Multi-core processors have brought parallel computing to desktop computers. Thus parallelization of serial programs
has become a mainstream programming task. In 2012 quad-core processors became standard for desktop computers, while
servers had 10+ core processors. Moore's law predicted that the number of cores per processor would double every
18–24 months.[12] By 2023 some processors had over hundred cores. Some designs having a mix of performance and
efficiency cores (such as ARM's big.LITTLE design) due to thermal and design constraints.[citation needed]
An operating system can ensure that different tasks and user programs are run in parallel on the available cores.
However, for a serial software program to take full advantage of the multi-core architecture the programmer needs to
restructure and parallelize the code. A speed-up of application software runtime will no longer be achieved through
frequency scaling, instead programmers will need to parallelize their software code to take advantage of the
increasing computing power of multicore architectures.[13]
Relevant laws